@article{HTP_to_accelerate_crop_breeding,
title = {High throughput phenotyping to accelerate crop breeding and monitoring of diseases in the field},
journal = {Current Opinion in Plant Biology},
volume = {38},
pages = {184-192},
year = {2017},
note = {38 Biotic interactions 2017},
issn = {1369-5266},
doi = {https://doi.org/10.1016/j.pbi.2017.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S1369526617300675},
author = {Nadia Shakoor and Scott Lee and Todd C Mockler},
abstract = {Effective implementation of technology that facilitates accurate and high-throughput screening of thousands of field-grown lines is critical for accelerating crop improvement and breeding strategies for higher yield and disease tolerance. Progress in the development of field-based high throughput phenotyping methods has advanced considerably in the last 10 years through technological progress in sensor development and high-performance computing. Here, we review recent advances in high throughput field phenotyping technologies designed to inform the genetics of quantitative traits, including crop yield and disease tolerance. Successful application of phenotyping platforms to advance crop breeding and identify and monitor disease requires: (1) high resolution of imaging and environmental sensors; (2) quality data products that facilitate computer vision, machine learning and GIS; (3) capacity infrastructure for data management and analysis; and (4) automated environmental data collection. Accelerated breeding for agriculturally relevant crop traits is key to the development of improved varieties and is critically dependent on high-resolution, high-throughput field-scale phenotyping technologies that can efficiently discriminate better performing lines within a larger population and across multiple environments.}
}
@article{Big_Data_Agriculture,
author = {Shakoor, Nadia and Northrup, Daniel and Murray, Seth and Mockler, Todd C.},
title = {Big Data Driven Agriculture: Big Data Analytics in Plant Breeding, Genomics, and the Use of Remote Sensing Technologies to Advance Crop Productivity},
journal = {The Plant Phenome Journal},
volume = {2},
number = {1},
pages = {180009},
doi = {https://doi.org/10.2135/tppj2018.12.0009},
url = {https://acsess.onlinelibrary.wiley.com/doi/abs/10.2135/tppj2018.12.0009},
eprint = {https://acsess.onlinelibrary.wiley.com/doi/pdf/10.2135/tppj2018.12.0009},
abstract = {Core Ideas Interdisciplinary efforts in high-throughput field phenotyping Linking proximal and remote field phenotyping Cyberinfrastructure for high-throughput field phenotyping Plant breeding and agronomy are labor-intensive sciences, and the success of these disciplines is critical to meet planetary challenges of food and water security for the world's growing population. Recent gains in sensor technology, remote sensing, robotics and autonomy, big data analytics, and genomics are being adopted by agricultural scientists for high-throughput phenotyping, precision agriculture, and crop-scouting platforms. These technological gains are ushering in an era of digital agriculture that should greatly enhance the capacity of plant breeders and agronomists. This report encompasses the priorities and recommendations that emerged from two USDA National Institute of Food and Agriculture (NIFA)-funded Big Data Driven Agriculture workshops held on 26–27 Feb. 2018 in Arlington, VA. The objectives of the workshops were to bring together diverse subject-matter experts in the represented disciplines of plant breeding, machine learning, remote sensing, and big data infrastructure and analytics to (i) explore how large and comprehensive datasets in plant breeding, genomics, remote sensing, and analytics will benefit agriculture; (ii) discuss strategies for creating a successful field phenotyping campaign and to determine protocols for the collection and analysis of agricultural big data; (iii) consider how to best engage the broader community of public and private plant breeders and agronomists to determine additional challenges, make wider use of the data, and ensure application of standardized methods to other datasets; and (iv) generate a report describing cross-cutting short- and long-term funding needs for continued success in this domain.},
year = {2019}
}

@Article{autoML_pipeline_HTP,
AUTHOR = {Koh, Joshua C.O. and Spangenberg, German and Kant, Surya},
TITLE = {Automated Machine Learning for High-Throughput Image-Based Plant Phenotyping},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {858},
URL = {https://www.mdpi.com/2072-4292/13/5/858},
ISSN = {2072-4292},
ABSTRACT = {Automated machine learning (AutoML) has been heralded as the next wave in artificial intelligence with its promise to deliver high-performance end-to-end machine learning pipelines with minimal effort from the user. However, despite AutoML showing great promise for computer vision tasks, to the best of our knowledge, no study has used AutoML for image-based plant phenotyping. To address this gap in knowledge, we examined the application of AutoML for image-based plant phenotyping using wheat zing assessment with unmanned aerial vehicle (UAV) imagery as an example. The performance of an open-source AutoML framework, AutoKeras, in image classification and regression tasks was compared to transfer learning using modern convolutional neural network (CNN) architectures. For image classification, which classified plot images as lodged or non-lodged, transfer learning with Xception and DenseNet-201 achieved the best classification accuracy of 93.2%, whereas AutoKeras had a 92.4% accuracy. For image regression, which predicted lodging scores from plot images, transfer learning with DenseNet-201 had the best performance (R2 = 0.8303, root mean-squared error (RMSE) = 9.55, mean absolute error (MAE) = 7.03, mean absolute percentage error (MAPE) = 12.54%), followed closely by AutoKeras (R2 = 0.8273, RMSE = 10.65, MAE = 8.24, MAPE = 13.87%). In both tasks, AutoKeras models had up to 40-fold faster inference times compared to the pretrained CNNs. AutoML has significant potential to enhance plant phenotyping capabilities applicable in crop breeding and precision agriculture.},
DOI = {10.3390/rs13050858}
}
@Article{grain_crop_yield_pred_review,
AUTHOR = {Yuan, Jianghao and Zhang, Yangliang and Zheng, Zuojun and Yao, Wei and Wang, Wensheng and Guo, Leifeng},
TITLE = {Grain Crop Yield Prediction Using Machine Learning Based on UAV Remote Sensing: A Systematic Literature Review},
JOURNAL = {Drones},
VOLUME = {8},
YEAR = {2024},
NUMBER = {10},
ARTICLE-NUMBER = {559},
URL = {https://www.mdpi.com/2504-446X/8/10/559},
ISSN = {2504-446X},
ABSTRACT = {Preharvest crop yield estimation is crucial for achieving food security and managing crop growth. Unmanned aerial vehicles (UAVs) can quickly and accurately acquire field crop growth data and are important mediums for collecting agricultural remote sensing data. With the rapid development of machine learning, especially deep learning, research on yield estimation based on UAV remote sensing data and machine learning has achieved excellent results. This paper systematically reviews the current research of yield estimation research based on UAV remote sensing and machine learning through a search of 76 articles, covering aspects such as the grain crops studied, research questions, data collection, feature selection, optimal yield estimation models, and optimal growth periods for yield estimation. Through visual and narrative analysis, the conclusion covers all the proposed research questions. Wheat, corn, rice, and soybeans are the main research objects, and the mechanisms of nitrogen fertilizer application, irrigation, crop variety diversity, and gene diversity have received widespread attention. In the modeling process, feature selection is the key to improving the robustness and accuracy of the model. Whether based on single modal features or multimodal features for yield estimation research, multispectral images are the main source of feature information. The optimal yield estimation model may vary depending on the selected features and the period of data collection, but random forest and convolutional neural networks still perform the best in most cases. Finally, this study delves into the challenges currently faced in terms of data volume, feature selection and optimization, determining the optimal growth period, algorithm selection and application, and the limitations of UAVs. Further research is needed in areas such as data augmentation, feature engineering, algorithm improvement, and real-time yield estimation in the future.},
DOI = {10.3390/drones8100559}
}
@article{Volpato_maturity,
author = {Volpato, Leonardo and Dobbels, Austin and Borem, Aluízio and Lorenz, Aaron Joel},
title = {Optimization of temporal UAS-based imagery analysis to estimate plant maturity date for soybean breeding},
journal = {The Plant Phenome Journal},
volume = {4},
number = {1},
pages = {e20018},
doi = {https://doi.org/10.1002/ppj2.20018},
url = {https://acsess.onlinelibrary.wiley.com/doi/abs/10.1002/ppj2.20018},
eprint = {https://acsess.onlinelibrary.wiley.com/doi/pdf/10.1002/ppj2.20018},
abstract = {Abstract Estimating the date of maturity of soybean breeding field plots is necessary for breeding line characterization and for informing yield comparisons among varieties. The main drawback of visually dating soybean maturity is the sheer scale of note recording entailed and the frequency at which these notes need to be taken. The overall aim of this study was to build upon prior work in using low-cost UAS-based RGB cameras to estimate soybean maturity date by examining the effect of vegetation index, summary statistic of the pixel values from each region of interest (plot), statistical model, and flight frequency. Maturity dates collected from five environments with 53 experimental trials (4,415 plots) were both visually dated and imaged using a RGB camera carried by a UAS. Using the mean greenness leaf index on each plot combined with LOESS regression, we achieved high correlations between ground and UAS-based estimates (r = 0.84–0.97). Precision, quantified by broad-sense heritability estimates, was greater for UAS-based dates in 29 of 53 field trials, and nearly equivalent in 11 more field trials. We found that 54\% of the significant deviations between ground and UAS-based estimates were caused by inaccurate UAS-based estimates, while errors in the ground-based estimates accounted for 46\% of the deviations. Reasons for these inaccurate estimates were attributed to lodging, presence of weeds, low germination, and within-line genetic heterogeneity in the plots. A detailed description of the analysis pipeline, a user-friendly R script, and all of the images and ground data have been made publicly available to help other researchers and breeders test and adopt these methods.},
year = {2021}
}
@software{zarr_python,
  author    = {{Zarr Development Team}},
  title     = {Zarr},
  year      = {2024},
  version   = {2.18.2},
  publisher = {Python Package Index},
  url       = {https://pypi.org/project/zarr/}
}